---
title: "Homework 5"
author: Ziyan Xu
date: 11/10/2022
output: 
  github_document:
    toc: true
---

This is my solution to HW5. 

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d()
scale_fill_discrete = scale_fill_viridis_d()

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

### Problem 2

#### Load and describe the raw dataset

```{r include = FALSE}
raw_df = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

This dataset contains `r nrow(raw_df)` rows and `r ncol(raw_df)` columns.

The data contains the location of the killing, whether an arrest was made, basic demographic information of each victim. There are victim info variables -- victim_last, victim_first, victim_race, victim_age and victim_sex, location variables -- city, state, lat and lon, and case info -- uid, reported_date and disposition.

#### Clean the dataset
```{r echo = FALSE, message = FALSE}
homicide_df = 
  raw_df %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved")) %>% 
  filter(city_state != "Tulsa, AL") %>% 
  relocate(city_state) %>% 
  group_by(city_state) %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    total = n())
homicide_df
```

#### Proportion of unsolved homicides

Run `prop.test` for Baltimore, MD.
```{r}
baltimore_summary = 
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")
baltimore_test =
  prop.test(
    pull(baltimore_summary, unsolved), 
    pull(baltimore_summary, total)) %>% 
  broom::tidy() %>% 
  select(estimate, starts_with("conf"))
baltimore_test
```

Run `prop.test` for each city.
```{r}
homicide_cities =
  homicide_df %>%
  mutate(
    test_results = map2(unsolved, total, prop.test),
    tidy_results = map(test_results, broom::tidy)) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))
homicide_cities
```

A scatterplot that showing the estimates and CIs for each city.

```{r, dpi = 300, echo=FALSE}
homicide_cities %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate, color = city_state)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.position = "none") +
  labs(
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides",
    title = "Estimated Proportion of Unsolved Homicides in Each City"
  )
```

### Problem 3

#### Create the function 

For $\mu = 0$, generate 5000 datasets from the model $X \sim N(0,5)$ with sample size of 30.

```{r}
set.seed(1)

norm_fun <- function(mu,n = 30, sigma = 5)
{
  sim_data = tibble(x = rnorm(n, mean = mu,sd = sigma),)
  sim_data %>%
    mutate(mu_hat = broom::tidy(t.test(sim_data))$estimate,
           p_value = broom::tidy(t.test(sim_data))$p.value) %>%
    select(-x) %>%
    distinct()
}

sim0_df = rerun(5000,norm_fun(mu = 0)) %>%
  bind_rows()
```

#### Simulation for mu = 0,1,2,3,4,5,6

```{r}
sim_df <- tibble(mu = c(0:6)) %>% 
  mutate(output_list = map(.x = mu, ~rerun(5000, norm_fun(mu = .x))),
         result_df = map(output_list,bind_rows)) %>%
  select(-output_list) %>%
  unnest(result_df) %>%
  mutate(Null_hypothesis = ifelse(p_value >= 0.05, "Fail to reject",'Reject'))  
  head(sim_df)
```

#### Make a plot to show association between effect size and power.

```{r message = F, warning = F }
sim_df %>%
  group_by(mu) %>%
  summarize(power = sum(Null_hypothesis == 'Reject')/n()) %>% 
  ggplot(aes(x = mu, y = power))+
  geom_point(size = 2)+
  geom_line(alpha = 0.6)+
  geom_smooth(alpha = 0.3,size = 0.5)+
  labs( title = "Association between effect size and power",
        x = "True value of μ",
        y = "Power") +
  theme(plot.title = element_text(hjust = .5))
```

As effect size increases, power increases, when effect size is high enough (in this case: mu >= 4), power is approximately equal to 1. 

#### Make a plot showing the average estimate of μ^ on the y axis and the true μ 

```{r message = F, warning = F}
sim_df %>%
  group_by(mu) %>%
  summarize(average_mu_hat = mean(mu_hat)) %>%
  ggplot(aes(x = mu, y = average_mu_hat))+
  geom_point(size = 2)+
  geom_line(alpha = 0.6)+
  labs( title = "Association between true μ and average estimate of μ^",
        x = "True value of μ",
        y = "Average estimate of μ^") +
  theme(plot.title = element_text(hjust = .5))
```

Clearly the average estimate of μ^ is approximately equal to true μ in this 5000 times simulation for μ = 0,1,2,3,4,5,6

#### Make a plot showing the average estimate of μ^ on the y axis and the true μ in samples for which the null was rejected

```{r message = F, warning = F}
sim_df %>%
  filter(Null_hypothesis == 'Reject') %>%
  group_by(mu) %>%
  summarize(average_mu_hat = mean(mu_hat)) %>%
  ggplot(aes(x = mu, y = average_mu_hat))+
  geom_point(size = 2)+
  geom_line(alpha = 0.6)+
  labs( title = "Association between true μ and average estimate of μ^ when H0 is rejected",
        x = "True value of μ",
        y = "Average estimate of μ^") +
  theme(plot.title = element_text(hjust = .5))
```

Across test for which the null hypothesis is rejected, when true μ = 0,1,2,3, which means that the true μ is relatively close to the null hypothesis(μ = 0), the average estimate of μ^ is not equal to true value of μ. This is because μ^ for those null hypothesis is rejected are biased from population mean since the proportion of rejecting μ = 0 is not high enough. In a special case when population mean = 1, μ^ needs to be larger to reject H0 (which is μ = 0), so average estimate of μ^ is above 2. 
  
When true μ is so large that the null hypothesis is always rejected, the average estimate of μ^ is approximately equal to true value of μ. This is because the average estimate of μ^ basically represent the estimate of μ in 5000 samples simulated from normal distribution, which equals to the population mean.

